---
title: "Chapter 9: Multiple linear regression"
author: "Alexis Mekueko"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    df_print: paged
    smooth_scroll: yes
    toc_collapsed: yes
    toc_depth: 5
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '5'
    toc_float: yes
    code_folding: "hide"
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## R Packages

```{r load-packages, message=FALSE}

library(tidyverse) #loading all library needed for this assignment
library(openintro)
#head(fastfood)
#library(readxl)
library(data.table)
#library(DT)
library(knitLatex)
library(knitr)
library(markdown)
library(rmarkdown)

#library(readr)
#library(plyr)
#library(dplyr)
library(stringr)
#library(XML)
#library(RCurl)
#library(jsonlite)
#library(httr)

#library(maps)
#library(dice)
# #library(VennDiagram)
# #library(help = "dice")
#ibrary(DBI)
#library(dbplyr)

# library(rstudioapi)
# library(RJDBC)
# library(odbc)
# library(RSQLite)
# #library(rvest)

#library(readtext)
#library(ggpubr)
#library(fitdistrplus)

library(plyr)
library(pdftools)
library(plotrix)
library(gplots)
library(tibble)
#library(moments)
#library(qualityTools)
#library(normalp)
#library(utils)
#library(MASS)
#library(qqplotr)
#library(stats)
library(statsr)
library(GGally)
#(DATA606)
```


Github Link: https://github.com/asmozo24/DATA606_Lab8

Web link: https://rpubs.com/amekueko/681421



### Multiple linear regression


## Grading the professor

Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, “Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity” by Hamermesh and Parker found that instructors who are viewed to be better looking receive higher instructional ratings.

Here, you will analyze the data from this study in order to learn what goes into a positive professor evaluation.

## Getting Started

#  Load packages

In this lab, you will explore and visualize the data using the tidyverse suite of packages. The data can be found in the companion package for OpenIntro resources, openintro.

Let’s load the packages.

This is the first time we’re using the GGally package. You will be using the ggpairs function from this package later in the lab.


## Creating a reproducible lab report

To create your new lab report, in RStudio, go to New File -> R Markdown… Then, choose From Template and then choose Lab Report for OpenIntro Statistics Labs from the list of templates.

## The data

The data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students rated the professors’ physical appearance. The result is a data frame where each row contains a different course and columns represent variables about the courses and professors. It’s called evals.

```{r }

glimpse(evals)
?evals

```

## Exploring the data

Exercice 1

Is this an observational study or an experiment? The original research question posed in the paper is whether beauty leads directly to the differences in course evaluations. Given the study design, is it possible to answer this question as it is phrased? If not, rephrase the question.

We have observations on 21 different variables, some categorical and some numerical. The meaning of each variable can be found by bringing up the help file:


##The data
The data we’re working with is in the openintro package and it’s called hfi, short for Human Freedom Index.

#Exercice 1

What are the dimensions of the dataset?
Answer: 1458 rows, 123 columns, data collected from 2008 to 2016, 



```{r}

# loading the dataset from the openIntro/data606 library
data(hfi)

#view hfi dataset details 
glimpse(hfi)

#view table in Rstudio
view(hfi)


```

#Exercice 2

What type of plot would you use to display the relationship between the personal freedom score, pf_score, and one of the other numerical variables? Plot this relationship using the variable pf_expression_control as the predictor. Does the relationship look linear? If you knew a country’s pf_expression_control, or its score out of 10, with 0 being the most, of political pressures and controls on media content, would you be comfortable using a linear model to predict the personal freedom score?

Answer: pf_score is numerical and one of the other numerical variables will make a 02 variables plot: I can use scatterplot of y against x [ plot(x,y)-cartesian or plot(x~y)-formula ] or box-and-whisker plot of y at levels of factor [plot(factor, y)] or heights from a vector of y values ([barplot(y)]). The relationship does show some linearity trend, but the data looks scatters. No, I would not be comfortable because on a large sample data, it is difficult to determine the type of plot. It is after carefully examining the variable with their observation that we can determine the type of plot which can fit.

```{r}

#hfi <- na.omit(hfi) # delete/remove the missings data because it is an imcomplete observation
# somehow, the removing NA delete the entire dataset. 

#plot to display the relationship between the personal freedom score, pf_score, and one of the other numerical variables pf_expression_control
plot(hfi$pf_score ~ hfi$pf_expression_control)

#scattered plot x = pf_expression_control, y = pf_score 
ggplot(hfi, aes(x=pf_expression_control, y=pf_score)) + geom_point() +
  ggtitle("Human Freedom Index \nScore per expression control") +
  xlab("Expression Control") + ylab("Score") +

# Change the color, the size and the face of
# the main title, x and y axis labels
 theme(
plot.title = element_text(color="red", size=14, face="bold.italic"),
axis.title.x = element_text(color="blue", size=12, face="bold"),
axis.title.y = element_text(color="#993333", size=12, face="bold"))


#ggplot(aes(x=pf_expression_control,y=pf_score,  main =" Human Freedom Index \nScore per expression control"  ),data= hfi)+
#  geom_point()

#If the relationship looks linear, we can quantify the strength of the relationship with the correlation coefficient.
hfi %>%
  summarise(cor(pf_expression_control, pf_score, use = "complete.obs"))
#Here, we set the use argument to “complete.obs” since there are some observations of NA.


```


###Sum of squared residuals

In this section, you will use an interactive function to investigate what we mean by “sum of squared residuals”. You will need to run this function in your console, not in your markdown document. Running the function also requires that the hfi dataset is loaded in your environment.

Think back to the way that we described the distribution of a single variable. Recall that we discussed characteristics such as center, spread, and shape. It’s also useful to be able to describe the relationship of two numerical variables, such as pf_expression_control and pf_score above.


#Exercice 3
Looking at your plot from the previous exercise, describe the relationship between these two variables. Make sure to discuss the form, direction, and strength of the relationship as well as any unusual observations.
Answer: Based on the corelation coefficient (0.7963894 ~ 0.8) , there is a strong corelation for a linear relation between pf_expression_control and pf_score. there are many points off which affect the linearity of the plot...

```{r}

#actually let's see a fit line
ggplot(hfi, aes(x=pf_expression_control, y=pf_score)) + geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Human Freedom Index \nScore per expression control") +
  xlab("Expression Control") + ylab("Score") +

# Change the color, the size and the face of
# the main title, x and y axis labels
 theme(
plot.title = element_text(color="red", size=14, face="bold.italic"),
axis.title.x = element_text(color="blue", size=12, face="bold"),
axis.title.y = element_text(color="#993333", size=12, face="bold"))

#ggplot(hfi, aes(pf_expression_control, pf_score)) + geom_point()+
#  geom_smooth(method = "lm", se = FALSE) # + facet_wrap(~cyl)

theme <- theme(
plot.title = element_text(color="red", size=14, face="bold.italic"),
axis.title.x = element_text(color="blue", size=12, face="bold"),
axis.title.y = element_text(color="#993333", size=12, face="bold"))


```



Just as you’ve used the mean and standard deviation to summarize a single variable, you can summarize the relationship between these two variables by finding the line that best follows their association. Use the following interactive function to select the line that you think does the best job of going through the cloud of points.



```{r}

#this isn't working, wondering if we should just remove rows with NA
# ok let/s remove the NA, I tried #hfi <- na.omit(hfi) # delete/remove the missings data because it is an imcomplete observation
# somehow, the removing NA delete the entire dataset. 
# this time , I will try removing NA in selected variable

hfi1 <- select(hfi, pf_expression_control, pf_score )
#checking how many values are missing in hfi
sum(is.na(hfi1))
hfi2 <- na.omit(hfi1)

# trying again to plot_ss
plot_ss(x = pf_expression_control, y = pf_score, data = hfi2)
    
# # trying to create a title function but somehow I cannot use it on plot_ss
# title1 <- title(main = "Human Freedom Index \nScore per expression control", 
#       xlab = "Expression Control", ylab = "Score",
#       cex.main = 2,   font.main= 4, col.main= "red",
#       cex.sub = 0.75, font.sub = 3, col.sub = "green",
#       col.lab ="darkblue"
#       )

#plot(hfi$pf_score ~ hfi$pf_expression_control)


```
After running this command, you’ll be prompted to click two points on the plot to define a line. Once you’ve done that, the line you specified will be shown in black and the residuals in blue. Note that there are 30 residuals, one for each of the 30 observations. Recall that the residuals are the difference between the observed values and the values predicted by the line: e1 = y_i - Y_i

The most common way to do linear regression is to select the line that minimizes the sum of squared residuals. To visualize the squared residuals, you can rerun the plot command and add the argument showSquares = TRUE.


```{r}

#plot_ss(x = hfi$pf_expression_control, y = hfi$pf_score, showSquares = TRUE)
plot_ss(x = pf_expression_control, y = pf_score, data = hfi2, showSquares = TRUE)

```

Note that the output from the plot_ss function provides you with the slope and intercept of your line as well as the sum of squares.

#Exercice 4
Using plot_ss, choose a line that does a good job of minimizing the sum of squares. Run the function several times. What was the smallest sum of squares that you got? How does it compare to your neighbors?
Answer: Sum of Squares:  952.153, I don't think it changes as I rerun. 


## The linear model

It is rather cumbersome to try to get the correct least squares line, i.e. the line that minimizes the sum of squared residuals, through trial and error. Instead, you can use the lm function in R to fit the linear model (a.k.a. regression line).
```{r}

m1 <- lm(pf_score ~ pf_expression_control, data = hfi2)

```
The first argument in the function lm is a formula that takes the form y ~ x. Here it can be read that we want to make a linear model of pf_score as a function of pf_expression_control. The second argument specifies that R should look in the hfi data frame to find the two variables.

The output of lm is an object that contains all of the information we need about the linear model that was just fit. We can access this information using the summary function.

```{r}

# display statistical details in m1
summary(m1)

```

Let’s consider this output piece by piece. First, the formula used to describe the model is shown at the top. After the formula you find the five-number summary of the residuals. The “Coefficients” table shown next is key; its first column displays the linear model’s y-intercept and the coefficient of at_bats. With this table, we can write down the least squares regression line for the linear model:

y^ = 4.61707 + 0.49143×pf_expression_control, b or y-intercept = 4.61707, slope or a-coeficient = 0.461707

One last piece of information we will discuss from the summary output is the Multiple R-squared, or more simply, R2
. The R2 value represents the proportion of variability in the response variable that is explained by the explanatory variable. For this model, 63.42% of the variability in runs is explained by at-bats.



# Exercice 5
Fit a new model that uses pf_expression_control to predict hf_score, or the total human freedom score. Using the estimates from the R output, write the equation of the regression line. What does the slope tell us in the context of the relationship between human freedom and the amount of political pressure on media content?

Answer: y^ = 5.153687 + 0.349862×pf_expression_control
slope: the human freedom score increases by 0.35 amount of political pressure on media content
intercept: when they started this study on political pressure on media content, the human freedom score has a reference of 5.153

```{r}
# Just replicate few lines from above, this time with new variable hf_score
hfi3 <- select(hfi, pf_expression_control, hf_score )
#checking how many values are missing in hfi
sum(is.na(hfi3))
hfi3 <- na.omit(hfi3)

m2 <- lm(hf_score ~ pf_expression_control, data = hfi3)
summary(m2)

```

## Prediction and prediction errors

Let’s create a scatterplot with the least squares line for m1 laid on top.

```{r}

# I did this already...was just looking at the fitting line
ggplot(data = hfi2, aes(x = pf_expression_control, y = pf_score))   +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE) +
   ggtitle("Human Freedom Index \nScore per expression control") +
  xlab("Expression Control") + ylab("Score") +
  theme


```

Here, we are literally adding a layer on top of our plot. geom_smooth creates the line by fitting a linear model. It can also show us the standard error se associated with our line, but we’ll suppress that for now.

This line can be used to predict y
at any value of x. When predictions are made for values of x that are beyond the range of the observed data, it is referred to as extrapolation and is not usually recommended. However, predictions made within the range of the data are more reliable. They’re also used to compute the residuals.

# Exercice 6 

If someone saw the least squares regression line and not the actual data, how would they predict a country’s personal freedom school for one with a 6.7 rating for pf_expression_control? Is this an overestimate or an underestimate, and by how much? In other words, what is the residual for this prediction?

Answer: If someone saw the least squares regression line and not the actual data, to predict a country’s personal freedom school for one with a 6.7 rating for pf_expression_control I would plot 6.7 and find the its correspondant on the least squares regression line which gives about ~7.25. This can be more acurated with redefiined scale on axis or axis increment value. However, I am actually wrong, when I filtered the value pf_expression_control = 6.7 from view(hfi2), there is no corresponding value for pf_score. we could use filter as showed in the code section below...observed (I approximated by using mean value) pf_score = 8.006315....Now what the equation would have given. 

Residual = Observed value - Fitted value. Linear 
residual = 8.006315-7.909651 = 0.09666408...very close or undersestimate 

```{r}

# filter value pf_score = 6.7
hfi6.7 <- hfi2 %>%
  filter(pf_expression_control > 6.7 & pf_expression_control <6.8)
# there is more than one corresponding values for pf_score for the same pf_expression_control. let's use mean
mean(hfi6.7$pf_score)
y = 4.61707 + 0.49143*6.7
y
residual = mean(hfi6.7$pf_score) - y
residual

# ggplot(data = hfi2, aes(x = pf_expression_control, y = pf_score), xaxt="n")+
#   geom_point() +
#   stat_smooth(method = "lm", se = FALSE) 
#   # axis(side=1, at=seq(0, 10, by =1), labels = seq(0, 23, 1)) #changing scale on the axis...does not work
#   # axis(side=2, at=seq(0, 10, by= 0.1))
# box() +
#    ggtitle("Human Freedom Index \nScore per expression control") +
#   xlab("Expression Control") + ylab("Score") +
#   theme


```
## Model diagnostics

To assess whether the linear model is reliable, we need to check for (1) linearity, (2) nearly normal residuals, and (3) constant variability.

Linearity: You already checked if the relationship between pf_score and `pf_expression_control’ is linear using a scatterplot. We should also verify this condition with a plot of the residuals vs. fitted (predicted) values.

```{r}

ggplot(data = m1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)+ # added for the fitted line
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


```

Notice here that m1 can also serve as a data set because stored within it are the fitted values (y^) and the residuals. Also note that we’re getting fancy with the code here. After creating the scatterplot on the first layer (first line of code), we overlay a horizontal dashed line at y=0 (to help us check whether residuals are distributed around 0), and we also reanme the axis labels to be more informative.

# Exercice 7
Is there any apparent pattern in the residuals plot? What does this indicate about the linearity of the relationship between the two variables?
Answer: based on the residuals plot , there is not pattern, and the variance of the error predicted value is constant across the x-axis which indicates there is likely a linear relationship between the two variables. 

## Nearly normal residuals: 
To check this condition, we can look at a histogram

```{r}

ggplot(data = m1, aes(x = .resid)) +
  geom_histogram(binwidth = 3) +  # residual of 25 is too high...max residual is around 2.5
  xlab("Residuals")

```

or a normal probability plot of the residuals.

```{r}

ggplot(data = m1, aes(sample = .resid)) +
  stat_qq()

```

Note that the syntax for making a normal probability plot is a bit different than what you’re used to seeing: we set sample equal to the residuals instead of x, and we set a statistical method qq, which stands for “quantile-quantile”, another name commonly used for normal probability plots.

## Exercice 8
Based on the histogram and the normal probability plot, does the nearly normal residuals condition appear to be met?
Answer: condition met


## Constant variability

# Exercice 9
Based on the residuals vs. fitted plot, does the constant variability condition appear to be met?
based on the residuals vs. fitted plot, there no pattern in the plot , which mean our assumption of constant variance (constant variability) condition appear to be met.

## More Practice

Choose another freedom variable and a variable you think would strongly correlate with it.. Produce a scatterplot of the two variables and fit a linear model. At a glance, does there seem to be a linear relationship?
Display the relationship between the personal freedom rank, pf_rank, and pf_association as the predictor.


```{r}

view(hfi)
# Just replicate few lines from above, this time with new variable hf_score  pf_association, pf_rank
hfi4 <- select(hfi, pf_association, pf_rank )
#checking how many values are missing in hfi
sum(is.na(hfi4))
hfi4 <- na.omit(hfi4)

#actually let's see a fit line
ggplot(hfi4, aes(x=pf_association, y=pf_rank)) + geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Human Freedom Index-Personal Freedom \nRank per Association") +
  xlab("Personal Freedom of Association") + ylab("Rank") + theme



```

How does this relationship compare to the relationship between pf_expression_control and pf_score? Use the R2 values from the two model summaries to compare. Does your independent variable seem to predict your dependent one better? Why or why not?
Answer:
coefficient of determination/coefficient of multiple determination/R-squared is a statistical measure of how close the data are to the fitted regression line. if R-squared = 100%, then the model explain all the variability of the response data around its mean. In other word, there is no variance in the dependent variable that can be explained by the predictors

pf_expression_control and pf_score: multiple R-squared = 0.6342 ~ 63.4%
pf_association and pf_rank: multiple R-Squared = 0.5804 ~58%

Based on the R-squared of the two relationships , there is about 5.4$  of a difference. The independent variable or predictor or explanatory (pf_association) does not seem to predict better the dependent variable or response variable or outcome variable. And that is the coefficient of correlation of the relationship between pf_association and pf_rank downhill(negative) -0.7618509 while the relationship between pf_expression_control and pf_score has a better coefficient of correlation uphill(positive) 0.7963894.


```{r}

m3 <- lm(pf_rank ~ pf_association, data = hfi4)
summary(m1)
summary(m3)
cor(hfi4$pf_association, hfi4$pf_rank)
cor(hfi2$pf_expression_control, hfi2$pf_score)


```

What’s one freedom relationship you were most surprised about and why? Display the model diagnostics for the regression model analyzing this relationship. 
Answer: pf_expression_internet, pf_expression...considering the penetration of internet and human rights world wide , we expect this relationship to be close to best fit. Howerver , it is not the case

```{r}


hfi5 <- select(hfi, pf_expression_internet, pf_expression)
#checking how many values are missing in hfi
sum(is.na(hfi5))
hfi5 <- na.omit(hfi5)

ggplot(hfi5, aes(x=pf_expression_internet, y=pf_expression)) + geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Human Freedom Index-Personal Freedom \nExpressionper Internet Expression ") +
  xlab("Personal Freedom of Internet Expression") + ylab("Personal Freedom Expression") + theme



m5 <- lm(pf_expression ~ pf_expression_internet, data = hfi5)

summary(m5)

cor(hfi5$pf_expression_internet, hfi5$pf_expression)


ggplot(data = m5, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")

ggplot(data = m5, aes(x = .resid)) +
  geom_histogram(binwidth = 3) +
  xlab("Residuals")

ggplot(data = m5, aes(sample = .resid)) +
  stat_qq()

```


